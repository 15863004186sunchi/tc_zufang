2017-03-04 21:29:59 [scrapy] INFO: Scrapy 1.2.0 started (bot: tc_zufang)
2017-03-04 21:29:59 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tc_zufang.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tc_zufang.spiders'], 'BOT_NAME': 'tc_zufang', 'COOKIES_ENABLED': False, 'SCHEDULER': 'tc_zufang.scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'logs/scrapy.log', 'DOWNLOAD_DELAY': 2}
2017-03-04 21:29:59 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-03-04 21:29:59 [tczufang] INFO: Reading start URLs from redis key 'tczufangCrawler:start_urls' (batch size: 16)
2017-03-04 21:29:59 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\scrapy_redis\dupefilter.py:7: ScrapyDeprecationWarning: Module `scrapy.dupefilter` is deprecated, use `scrapy.dupefilters` instead
  from scrapy.dupefilter import BaseDupeFilter

2017-03-04 21:29:59 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\rotate_useragent_dowmloadmiddleware.py:5: ScrapyDeprecationWarning: Module `scrapy.contrib.downloadermiddleware.useragent` is deprecated, use `scrapy.downloadermiddlewares.useragent` instead
  from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware

2017-03-04 21:30:00 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'tc_zufang.rotate_useragent_dowmloadmiddleware.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-04 21:30:00 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-04 21:30:00 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\mongodb_pipeline.py:3: ScrapyDeprecationWarning: Module `scrapy.log` has been deprecated, Scrapy now relies on the builtin Python library for logging. Read the updated logging entry in the documentation to learn more.
  from scrapy import log

2017-03-04 21:30:00 [scrapy] INFO: Enabled item pipelines:
['tc_zufang.mongodb_pipeline.SingleMongodbPipeline']
2017-03-04 21:30:00 [scrapy] INFO: Spider opened
2017-03-04 21:30:00 [tczufang] DEBUG: Resuming crawl (28 requests scheduled)
2017-03-04 21:30:00 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-04 21:30:00 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-03-04 21:30:00 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/robots.txt> (referer: None)
2017-03-04 21:30:02 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/28765303515847x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:03 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\mongodb_pipeline.py:42: ScrapyDeprecationWarning: log.msg has been deprecated, create a python logger and log through it instead
  level=log.DEBUG, spider=spider)

2017-03-04 21:30:03 [scrapy] DEBUG: Item 58bac15bd280ff14ec8d5d20 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:03 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/28765303515847x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe5\x8d\x83\xe7\x81\xaf\xe6\xb9\x96',
 'city': 'fs',
 'community': '\xe4\xb8\x87\xe7\xa7\x91\xe9\x87\x91\xe8\x89\xb2\xe9\xa2\x86\xe5\x9f\x9f',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1400',
 'pub_time': u'2017-02-11 18:15:02',
 'targeturl': 'http://fs.58.com/zufang/28765303515847x.shtml',
 'title': '\xe4\xb8\x87\xe7\xa7\x91\xe9\x87\x91\xe8\x89\xb2\xe9\xa2\x86\xe5\x9f\x9f \xe5\x8f\x8c\xe5\x9c\xb0\xe9\x93\x81+\xe8\xbd\xbb\xe8\xbd\xa8\xe9\x85\x8d\xe5\xa5\x97\xe6\x88\x90\xe7\x86\x9f \xe7\xb2\xbe\xe8\xa3\x85\xe8\xb1\xaa\xe5\x8d\x8e \xe6\x8b\xa7\xe5\x8c\x85\xe5\x85\xa5\xe4\xbd\x8f'}
2017-03-04 21:30:05 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/28776664473519x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:05 [scrapy] DEBUG: Item 58bac15dd280ff14ec8d5d21 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:05 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/28776664473519x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe5\xa4\xa7\xe8\x89\xaf\xe4\xb8\xad\xe5\x8c\xba',
 'city': 'fs',
 'community': '\xe4\xb8\x9c\xe8\x8b\x91\xe6\x96\xb0\xe6\x9d\x91',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1350',
 'pub_time': u'2017-02-09 09:34:02',
 'targeturl': 'http://fs.58.com/zufang/28776664473519x.shtml',
 'title': 'N\xe5\xa4\xa7\xe8\x89\xaf\xe4\xb8\xad\xe5\x8c\xba\xe4\xb8\x9c\xe8\x8b\x91\xe6\x96\xb0\xe6\x9d\x913\xe6\x88\xbf\xe5\xae\xb6\xe7\xa7\x81\xe5\xae\xb6\xe7\x94\xb5\xe9\xbd\x90\xe6\x8b\x8e\xe5\x8c\x85\xe5\x85\xa5\xe4\xbd\x8f'}
2017-03-04 21:30:08 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/28786490999209x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:08 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/28786490999209x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 53, in parse_detail
    tczufangItem['pub_time'] =re.findall(r'\d+\-\d+\-\d+\s+\d+\:\d+\:\d+',raw_time)[0]
  File "c:\users\seven\appdata\local\programs\python\python27\lib\re.py", line 181, in findall
    return _compile(pattern, flags).findall(string)
TypeError: expected string or buffer
2017-03-04 21:30:11 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/28789569735369x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:11 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/28789569735369x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 60, in parse_detail
    area=response_selector.xpath(u'//ul[contains(@class,"f14")]/li/span/a[contains(@class,"c_333")]/text()').extract()[1]
IndexError: list index out of range
2017-03-04 21:30:14 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/28994997011903x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:14 [scrapy] DEBUG: Item 58bac166d280ff14ec8d5d22 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:14 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/28994997011903x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe9\xbe\x99\xe6\xb1\x9f',
 'city': 'fs',
 'community': '\xe5\xba\xb7\xe4\xb9\x90\xe5\xb1\x85',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1500',
 'pub_time': u'2017-02-08 11:10:02',
 'targeturl': 'http://fs.58.com/zufang/28994997011903x.shtml',
 'title': '\xe5\xae\xb6\xe7\x94\xb5\xe9\xbd\x90\xe5\x85\xa8 \xe5\xb9\xb2\xe5\x87\x80\xe6\x95\xb4\xe6\xb4\x81 \xe9\x9a\x8f\xe6\x97\xb6\xe6\x8b\x8e\xe5\x8c\x85\xe5\x85\xa5\xe4\xbd\x8f \xe6\xac\xa2\xe8\xbf\x8e\xe6\x9d\xa5\xe7\xa7\x9f'}
2017-03-04 21:30:17 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29027629668279x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:17 [scrapy] DEBUG: Item 58bac169d280ff14ec8d5d23 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:17 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29027629668279x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe5\xae\xb9\xe6\xa1\x82',
 'city': 'fs',
 'community': '\xe4\xbd\x9b\xe7\xbd\x97\xe4\xbc\xa6\xe6\x96\xaf\xe6\x9f\x8f\xe6\x82\xa6\xe6\xb9\xbe',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'2500',
 'pub_time': u'2017-02-11 09:59:02',
 'targeturl': 'http://fs.58.com/zufang/29027629668279x.shtml',
 'title': '\xe6\x9f\x8f\xe6\x82\xa6\xe6\xb9\xbe \xe4\xb8\x89\xe6\x88\xbf\xe5\xb8\xa6\xe8\xa3\x85\xe4\xbf\xae\xe8\xbf\x9e\xe5\xae\xb6\xe7\xa7\x81\xe5\xae\xb6\xe7\x94\xb5\xe5\x87\xba\xe7\xa7\x9f'}
2017-03-04 21:30:19 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29030720223535x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:19 [scrapy] DEBUG: Item 58bac16bd280ff14ec8d5d24 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:19 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29030720223535x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe5\x8d\x83\xe7\x81\xaf\xe6\xb9\x96',
 'city': 'fs',
 'community': '\xe4\xbf\x8a\xe9\x9b\x85\xe8\x8a\xb1\xe5\x9b\xad',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'2500',
 'pub_time': u'2017-02-11 16:41:02',
 'targeturl': 'http://fs.58.com/zufang/29030720223535x.shtml',
 'title': '\xe4\xbf\x8a\xe9\x9b\x85\xe8\x8a\xb1\xe5\x9b\xad\xe4\xb8\x89\xe6\x88\xbf\xe4\xba\x8c\xe5\x8e\x85\xe4\xba\x8c\xe5\x8d\xab\xef\xbc\x8c\xe8\xa3\x85\xe4\xbf\xae\xe6\x96\xb0\xe5\x87\x80\xef\xbc\x8c\xe5\xae\xb6\xe7\x94\xb5\xe9\xbd\x90\xe5\x85\xa8\xef\xbc\x8c\xe4\xb8\xad\xe9\x97\xb4\xe6\xa5\xbc\xe5\xb1\x82\xef\xbc\x81'}
2017-03-04 21:30:22 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29041747421000x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:22 [scrapy] DEBUG: Item 58bac16ed280ff14ec8d5d25 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:22 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29041747421000x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe7\x8b\xae\xe5\xb1\xb1',
 'city': 'fs',
 'community': '\xe6\xb5\xb7\xe9\x80\xb8\xe9\x94\xa6\xe7\xbb\xa3\xe6\xa1\x83\xe5\x9b\xad',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'2000',
 'pub_time': u'2017-02-12 16:37:02',
 'targeturl': 'http://fs.58.com/zufang/29041747421000x.shtml',
 'title': '\xe9\x94\xa6\xe7\xbb\xa3\xe6\xa1\x83\xe5\x9b\xad \xe7\xb2\xbe\xe8\xa3\x85\xe4\xbf\xae \xe9\x85\x8d\xe5\xa5\x97\xe9\xbd\x90\xe5\x85\xa8\xe6\x8b\x8e\xe5\x8c\x85\xe5\x8d\xb3\xe4\xbd\x8f \xe7\x8e\xaf\xe5\xa2\x83\xe8\x88\x92\xe9\x80\x82 \xe6\xac\xa2\xe8\xbf\x8e\xe6\x9d\xa5\xe7\x94\xb5\xe5\x92\xa8\xe8\xaf\xa2'}
2017-03-04 21:30:24 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29052933159731x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:24 [scrapy] DEBUG: Item 58bac170d280ff14ec8d5d26 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:24 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29052933159731x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe5\x9f\x8e\xe5\xb8\x82\xe5\xb9\xbf\xe5\x9c\xba',
 'city': 'fs',
 'community': '\xe5\x8d\x97\xe6\xb5\xb7\xe4\xb8\x87\xe7\xa7\x91\xe5\xb9\xbf\xe5\x9c\xba',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'3000',
 'pub_time': u'2017-02-13 16:54:02',
 'targeturl': 'http://fs.58.com/zufang/29052933159731x.shtml',
 'title': '\xe5\x8d\x97\xe6\xb5\xb7\xe4\xb8\x87\xe7\xa7\x91\xe5\xb9\xbf\xe5\x9c\xba \xe5\x8d\x97\xe5\x8c\x97\xe5\xaf\xb9\xe6\xb5\x81 \xe5\x85\xa8\xe6\x96\xb0\xe8\xa3\x85\xe4\xbf\xae \xe9\xab\x98\xe6\xa1\xa3\xe5\xae\xb6\xe5\x85\xb7 \xe8\xbf\x91\xe5\x9c\xb0\xe9\x93\x81 \xe9\xa3\x8e\xe6\x99\xaf\xe5\xa5\xbd'}
2017-03-04 21:30:26 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29066875435088x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:26 [scrapy] DEBUG: Item 58bac172d280ff14ec8d5d27 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:26 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29066875435088x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe5\x8c\x97\xe6\xbb\x98',
 'city': 'fs',
 'community': '\xe7\xbe\x8e\xe7\x9a\x84\xe6\x96\xb0\xe6\xb5\xb7\xe5\xb2\xb8',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'900',
 'pub_time': u'2017-02-14 23:09:02',
 'targeturl': 'http://fs.58.com/zufang/29066875435088x.shtml',
 'title': '\xe9\xa1\xba\xe5\xbe\xb7\xe5\x8c\x97\xe6\xbb\x98 \xe7\xbe\x8e\xe7\x9a\x84\xe6\x96\xb0\xe6\xb5\xb7\xe5\xb2\xb8 1\xe5\xae\xa4 1\xe5\x8e\x85 33\xe5\xb9\xb3\xe7\xb1\xb3'}
2017-03-04 21:30:28 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29075621409580x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:28 [scrapy] DEBUG: Item 58bac174d280ff14ec8d5d28 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:28 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29075621409580x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe9\xbb\x84\xe5\xb2\x90',
 'city': 'fs',
 'community': '\xe4\xb8\xad\xe5\x8d\x97\xe8\x8a\xb1\xe5\x9b\xad',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1700',
 'pub_time': u'2017-02-15 18:08:02',
 'targeturl': 'http://fs.58.com/zufang/29075621409580x.shtml',
 'title': '\xe9\xbb\x84\xe5\xb2\x902\xe5\xae\xa41700\xe5\x85\x83\xe5\xa5\xbd\xe6\x88\xbf\xe5\x87\xba\xe7\xa7\x9f,\xe5\xb1\x85\xe4\xbd\x8f\xe8\x88\x92\xe9\x80\x82,\xe5\xb9\xb2\xe5\x87\x80\xe6\x95\xb4\xe6\xb4\x81,\xe9\x9a\x8f\xe6\x97\xb6\xe5\x85\xa5\xe4\xbd\x8f'}
2017-03-04 21:30:31 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29087117132476x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:31 [scrapy] DEBUG: Item 58bac177d280ff14ec8d5d29 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:31 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29087117132476x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe5\x9f\x8e\xe5\xb8\x82\xe5\xb9\xbf\xe5\x9c\xba',
 'city': 'fs',
 'community': '\xe5\x8d\x97\xe6\xb5\xb7\xe9\xa2\x90\xe6\x99\xaf\xe5\x9b\xad',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'3000',
 'pub_time': u'2017-02-27 19:15:02',
 'targeturl': 'http://fs.58.com/zufang/29087117132476x.shtml',
 'title': '\xe5\x8d\x97\xe6\xb5\xb7\xe9\xa2\x90\xe6\x99\xaf\xe5\x9b\xad \xe8\x8b\x8f\xe5\xb7\x9e\xe9\x9d\x93\xe5\x9b\xad\xe6\x9e\x97 \xe5\xae\x9c\xe5\xb1\x85\xe5\x8d\x95\xe4\xbd\x8d \xe7\xb2\xbe\xe8\xa3\x85\xe4\xb8\x89\xe6\x88\xbf \xe5\x8d\x97\xe5\x8c\x97\xe5\xaf\xb9\xe6\xb5\x81'}
2017-03-04 21:30:32 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29199346855721x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:32 [scrapy] DEBUG: Item 58bac178d280ff14ec8d5d2a wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:32 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29199346855721x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe6\x9d\xbe\xe5\xb2\x97',
 'city': 'fs',
 'community': '\xe6\xb5\xb7\xe9\x80\xb8\xe7\x9b\x9b\xe4\xb8\x96\xe6\xa1\x83\xe5\x9b\xad',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1100',
 'pub_time': u'2017-02-26 22:38:02',
 'targeturl': 'http://fs.58.com/zufang/29199346855721x.shtml',
 'title': '\xe6\x9d\xbe\xe5\xb2\x97 \xe7\x9b\x9b\xe4\xb8\x96\xe6\xa1\x83\xe5\x9b\xad \xe5\x8d\x95\xe8\xba\xab\xe5\x85\xac\xe5\xaf\x93 \xe5\xae\xb6\xe7\xa7\x81\xe7\x94\xb5\xe9\xbd\x90\xe5\x85\xa8 \xe9\x85\x8d\xe5\xa5\x97\xe5\xae\x8c\xe5\x96\x84 \xe4\xbb\xb7\xe9\x92\xb1\xe5\xae\x9e\xe6\x83\xa0'}
2017-03-04 21:30:34 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29206246622254x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:34 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/29206246622254x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 53, in parse_detail
    tczufangItem['pub_time'] =re.findall(r'\d+\-\d+\-\d+\s+\d+\:\d+\:\d+',raw_time)[0]
  File "c:\users\seven\appdata\local\programs\python\python27\lib\re.py", line 181, in findall
    return _compile(pattern, flags).findall(string)
TypeError: expected string or buffer
2017-03-04 21:30:36 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29209155394491x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:36 [scrapy] DEBUG: Item 58bac17cd280ff14ec8d5d2b wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:36 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29209155394491x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe5\x9d\x87\xe5\xae\x89',
 'city': 'fs',
 'community': '\xe5\x9d\x87\xe5\x9f\x8e\xe4\xb8\x8a\xe5\xb2\x9b\xe6\xb0\xb4\xe5\xb2\xb8',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'4000',
 'pub_time': u'2017-03-04 20:01:03',
 'targeturl': 'http://fs.58.com/zufang/29209155394491x.shtml',
 'title': '\xe5\x9d\x87\xe5\x9f\x8e\xe4\xb8\x8a\xe5\xb2\x9b\xe6\xb0\xb4\xe5\xb2\xb8 4\xe5\xae\xa43\xe5\x8e\x853\xe5\x8d\xab(\xe4\xb8\xaa\xe4\xba\xba)'}
2017-03-04 21:30:37 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29217073015729x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:37 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/29217073015729x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 53, in parse_detail
    tczufangItem['pub_time'] =re.findall(r'\d+\-\d+\-\d+\s+\d+\:\d+\:\d+',raw_time)[0]
  File "c:\users\seven\appdata\local\programs\python\python27\lib\re.py", line 181, in findall
    return _compile(pattern, flags).findall(string)
TypeError: expected string or buffer
2017-03-04 21:30:40 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29217369733571x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:40 [scrapy] DEBUG: Item 58bac180d280ff14ec8d5d2c wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:40 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29217369733571x.shtml>
{'area': '\xe7\xa6\x85\xe5\x9f\x8e-\xe5\xad\xa3\xe5\x8d\x8e\xe8\xb7\xaf',
 'city': 'fs',
 'community': '\xe4\xb9\x9d\xe9\xbc\x8e\xe5\x9b\xbd\xe9\x99\x85\xe5\x9f\x8e',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'2400',
 'pub_time': u'2017-02-28 13:45:02',
 'targeturl': 'http://fs.58.com/zufang/29217369733571x.shtml',
 'title': '\xe5\xad\xa3\xe5\x8d\x8e\xe5\x85\xad\xe8\xb7\xaf\xe4\xb9\x9d\xe9\xbc\x8e\xe5\x9b\xbd\xe9\x99\x85\xe5\x85\xac\xe5\xaf\x93 \xe6\xb8\xa9\xe9\xa6\xa82\xe6\x88\xbf \xe9\x85\x8d\xe5\xa5\x97\xe6\x88\x90\xe7\x86\x9f \xe4\xba\xa4\xe9\x80\x9a\xe6\x96\xb9\xe4\xbe\xbf \xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87'}
2017-03-04 21:30:42 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29219751780651x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:43 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/29219751780651x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 53, in parse_detail
    tczufangItem['pub_time'] =re.findall(r'\d+\-\d+\-\d+\s+\d+\:\d+\:\d+',raw_time)[0]
  File "c:\users\seven\appdata\local\programs\python\python27\lib\re.py", line 181, in findall
    return _compile(pattern, flags).findall(string)
TypeError: expected string or buffer
2017-03-04 21:30:45 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29228499167932x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:46 [scrapy] DEBUG: Item 58bac186d280ff14ec8d5d2d wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:46 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29228499167932x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe5\xae\xb9\xe6\xa1\x82',
 'city': 'fs',
 'community': '\xe6\xa1\xa5\xe9\x82\xa6\xe5\x85\xac\xe5\xaf\x93',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1200',
 'pub_time': u'2017-03-01 13:54:03',
 'targeturl': 'http://fs.58.com/zufang/29228499167932x.shtml',
 'title': '\xe5\xae\xb9\xe6\xa1\x82\xe6\xa1\xa5\xe9\x82\xa6\xe5\x85\xac\xe5\xaf\x93\xe8\xb1\xaa\xe8\xa3\x85\xe6\x9c\x9b\xe6\xb1\x9f\xe9\x80\x82\xe5\x90\x88\xe5\xae\xb9\xe6\xa1\x82\xe5\xa4\xa7\xe8\x89\xaf\xe4\xb8\x8a\xe7\x8f\xad\xe4\xb8\x80\xe6\x97\x8f'}
2017-03-04 21:30:48 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29228641427788x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:48 [scrapy] DEBUG: Item 58bac188d280ff14ec8d5d2e wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:48 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29228641427788x.shtml>
{'area': '\xe9\xa1\xba\xe5\xbe\xb7-\xe4\xb9\x90\xe4\xbb\x8e',
 'city': 'fs',
 'community': '\xe6\x98\x9f\xe5\x85\x89\xe5\xb9\xbf\xe5\x9c\xba',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1499',
 'pub_time': u'2017-03-01 14:12:03',
 'targeturl': 'http://fs.58.com/zufang/29228641427788x.shtml',
 'title': '\xe4\xb9\x90\xe4\xbb\x8e\xe6\x98\x9f\xe5\x85\x89\xe5\xb9\xbf\xe5\x9c\xba \xe9\xab\x98\xe6\xa1\xa3\xe4\xb8\x80\xe6\x88\xbf\xe4\xb8\x80\xe5\x8e\x85\xe5\x85\xac\xe5\xaf\x93 \xe9\xa9\xac\xe4\xb8\x8a\xe7\x9c\x8b\xe6\x88\xbf'}
2017-03-04 21:30:51 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29231597056567x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:51 [scrapy] DEBUG: Item 58bac18bd280ff14ec8d5d2f wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:51 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29231597056567x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe9\xbb\x84\xe5\xb2\x90',
 'city': 'fs',
 'community': '\xe7\x99\xbd\xe6\xb2\x99',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1600',
 'pub_time': u'2017-03-04 20:03:03',
 'targeturl': 'http://fs.58.com/zufang/29231597056567x.shtml',
 'title': '\xe6\x96\xb0\xe8\xa3\x85\xe4\xbf\xae2\xe5\x8e\x853\xe6\x88\xbf+\xe5\xb7\xa5\xe4\xbd\x9c\xe5\xae\xa4\xe5\x8f\xaf\xe6\xb3\xa8\xe5\x86\x8c\xe5\x95\x86\xe4\xbd\x8f\xe4\xba\x8c\xe7\x94\xa8\xe8\xbf\x91\xe5\xb9\xbf\xe5\xb7\x9e\xe4\xba\xa4\xe9\x80\x9a\xe6\x96\xb9\xe4\xbe\xbf(\xe4\xb8\xaa\xe4\xba\xba)'}
2017-03-04 21:30:52 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29242636165442x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:53 [scrapy] DEBUG: Item 58bac18dd280ff14ec8d5d30 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:53 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29242636165442x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe5\x8d\x83\xe7\x81\xaf\xe6\xb9\x96',
 'city': 'fs',
 'community': '\xe4\xb8\x87\xe8\xbe\xbe\xe5\x8d\x8e\xe5\xba\x9c',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'3200',
 'pub_time': u'2017-03-02 20:34:03',
 'targeturl': 'http://fs.58.com/zufang/29242636165442x.shtml',
 'title': '\xe4\xb8\x87\xe8\xbe\xbe\xe5\x8d\x8e\xe5\xba\x9c \xe7\xb2\xbe\xe8\xa3\x85\xe4\xb8\x89\xe6\x88\xbf \xe5\xae\xb6\xe7\x94\xb5\xe9\xbd\x90\xe5\x85\xa8 \xe6\x8b\x8e\xe5\x8c\x85\xe5\x85\xa5\xe4\xbd\x8f \xe7\x9c\x8b\xe6\x88\xbf\xe6\x96\xb9\xe4\xbe\xbf'}
2017-03-04 21:30:55 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29251021278540x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:55 [scrapy] DEBUG: Item 58bac18fd280ff14ec8d5d31 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:30:55 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29251021278540x.shtml>
{'area': '\xe7\xa6\x85\xe5\x9f\x8e-\xe7\x9f\xb3\xe6\xb9\xbe',
 'city': 'fs',
 'community': '\xe7\x9b\x88\xe7\xbf\xa0\xe5\x8d\x8e\xe8\x8b\x91',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'2000',
 'pub_time': u'2017-03-03 14:46:03',
 'targeturl': 'http://fs.58.com/zufang/29251021278540x.shtml',
 'title': '\xe7\x9f\xb3\xe6\xb9\xbe\xe7\x9b\x88\xe7\xbf\xa0\xe5\x8d\x8e\xe8\x8b\x91 3\xe6\x96\xb92\xe5\x8e\x85 \xe5\x8f\xaa\xe9\x9c\x802000 \xe6\x8b\x8e\xe5\x8c\x85\xe5\x85\xa5\xe4\xbd\x8f'}
2017-03-04 21:30:57 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29263675966009x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:30:57 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/29263675966009x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 53, in parse_detail
    tczufangItem['pub_time'] =re.findall(r'\d+\-\d+\-\d+\s+\d+\:\d+\:\d+',raw_time)[0]
  File "c:\users\seven\appdata\local\programs\python\python27\lib\re.py", line 181, in findall
    return _compile(pattern, flags).findall(string)
TypeError: expected string or buffer
2017-03-04 21:30:59 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29264475299520x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:31:00 [scrapy] ERROR: Spider error processing <GET http://fs.58.com/zufang/29264475299520x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
Traceback (most recent call last):
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 29, in process_spider_output
    for x in result:
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "c:\users\seven\appdata\local\programs\python\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "F:\githubcode\tc_zufang\tc_zufang\spiders\tczufang_detail_spider.py", line 53, in parse_detail
    tczufangItem['pub_time'] =re.findall(r'\d+\-\d+\-\d+\s+\d+\:\d+\:\d+',raw_time)[0]
  File "c:\users\seven\appdata\local\programs\python\python27\lib\re.py", line 181, in findall
    return _compile(pattern, flags).findall(string)
TypeError: expected string or buffer
2017-03-04 21:31:00 [scrapy] INFO: Crawled 26 pages (at 26 pages/min), scraped 18 items (at 18 items/min)
2017-03-04 21:31:02 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29264476402254x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:31:02 [scrapy] DEBUG: Item 58bac196d280ff14ec8d5d32 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:31:02 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29264476402254x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe7\x8b\xae\xe5\xb1\xb1',
 'city': 'fs',
 'community': '\xe4\xbf\x8a\xe5\x8d\x9a\xe8\x8b\x91',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'1850',
 'pub_time': u'2017-03-04 19:58:03',
 'targeturl': 'http://fs.58.com/zufang/29264476402254x.shtml',
 'title': '\xe7\x8b\xae\xe5\xb1\xb1-\xe5\x8d\x97\xe6\xb5\xb7\xe4\xbf\x8a\xe5\x8d\x9a\xe8\x8b\x912\xe5\xae\xa42\xe5\x8e\x852\xe5\x8d\xab1850\xe5\x85\x83(\xe4\xb8\xaa\xe4\xba\xba)'}
2017-03-04 21:31:04 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29264514250674x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:31:05 [scrapy] DEBUG: Item 58bac199d280ff14ec8d5d33 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:31:05 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29264514250674x.shtml>
{'area': '\xe7\xa6\x85\xe5\x9f\x8e-\xe5\xbc\xa0\xe6\xa7\x8e',
 'city': 'fs',
 'community': '\xe5\xbc\xa0\xe6\xa7\x8e\xe5\xa4\xa7\xe5\xaf\x8c',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'500',
 'pub_time': u'2017-03-04 20:03:03',
 'targeturl': 'http://fs.58.com/zufang/29264514250674x.shtml',
 'title': '\xe5\xbc\xa0\xe6\xa7\x8e\xe5\xa4\xa7\xe5\xaf\x8c\xe7\x94\xb5\xe6\xa2\xaf\xe6\x88\xbf\xe5\xb8\xa6\xe5\xae\xb6\xe5\x85\xb7\xe4\xb8\x80\xe5\xae\xa4\xe4\xb8\x80\xe5\x8e\x85 \xe7\xb2\xbe\xe8\xa3\x85\xe4\xbf\xae \xe4\xbd\x8e\xe4\xbb\xb7\xe7\x9b\xb4\xe7\xa7\x9f\xe9\x9d\x9e\xe8\xaf\x9a\xe5\x8b\xbf\xe6\x89\xb0(\xe4\xb8\xaa\xe4\xba\xba)'}
2017-03-04 21:31:07 [scrapy] DEBUG: Crawled (200) <GET http://fs.58.com/zufang/29264520575543x.shtml> (referer: http://fs.58.com/chuzu/pn9/)
2017-03-04 21:31:07 [scrapy] DEBUG: Item 58bac19bd280ff14ec8d5d34 wrote to MongoDB database zufang_fs/book_detail
2017-03-04 21:31:07 [scrapy] DEBUG: Scraped from <200 http://fs.58.com/zufang/29264520575543x.shtml>
{'area': '\xe5\x8d\x97\xe6\xb5\xb7-\xe5\xa4\xa9\xe4\xbd\x91',
 'city': 'fs',
 'community': '\xe6\xa1\x82\xe8\x8a\xb1\xe5\x9b\xad\xe8\xa5\xbf\xe5\x8c\xba',
 'method': '\xe6\x95\xb4\xe7\xa7\x9f',
 'money': u'2000',
 'pub_time': u'2017-03-04 20:27:03',
 'targeturl': 'http://fs.58.com/zufang/29264520575543x.shtml',
 'title': '\xe6\xa1\x82\xe8\x8a\xb1\xe5\x9b\xad\xe8\xa5\xbf\xe5\x8c\xba 3\xe5\xae\xa42\xe5\x8e\x851\xe5\x8d\xab \xe4\xb8\xaa\xe4\xba\xba\xe5\x87\xba\xe7\xa7\x9f(\xe4\xb8\xaa\xe4\xba\xba)'}
2017-03-04 21:32:00 [scrapy] INFO: Crawled 29 pages (at 3 pages/min), scraped 21 items (at 3 items/min)
2017-03-04 21:33:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:34:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:35:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:36:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:37:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:38:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:39:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:40:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:41:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:42:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:43:00 [scrapy] INFO: Crawled 29 pages (at 0 pages/min), scraped 21 items (at 0 items/min)
2017-03-04 21:43:52 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2017-03-04 21:43:52 [scrapy] INFO: Closing spider (shutdown)
2017-03-04 21:43:52 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 7861,
 'downloader/request_count': 29,
 'downloader/request_method_count/GET': 29,
 'downloader/response_bytes': 260341,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 29,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2017, 3, 4, 13, 43, 52, 324000),
 'item_scraped_count': 21,
 'log_count/DEBUG': 73,
 'log_count/ERROR': 7,
 'log_count/INFO': 22,
 'log_count/WARNING': 4,
 'response_received_count': 29,
 'scheduler/dequeued/redis': 28,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 6,
 'start_time': datetime.datetime(2017, 3, 4, 13, 30, 0, 318000)}
2017-03-04 21:43:52 [scrapy] INFO: Spider closed (shutdown)
2017-03-04 21:53:04 [scrapy] INFO: Scrapy 1.2.0 started (bot: tc_zufang)
2017-03-04 21:53:04 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tc_zufang.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['tc_zufang.spiders'], 'BOT_NAME': 'tc_zufang', 'COOKIES_ENABLED': False, 'SCHEDULER': 'tc_zufang.scrapy_redis.scheduler.Scheduler', 'LOG_FILE': 'logs/scrapy.log', 'DOWNLOAD_DELAY': 2}
2017-03-04 21:53:05 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2017-03-04 21:53:05 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\scrapy_redis\dupefilter.py:7: ScrapyDeprecationWarning: Module `scrapy.dupefilter` is deprecated, use `scrapy.dupefilters` instead
  from scrapy.dupefilter import BaseDupeFilter

2017-03-04 21:53:05 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\rotate_useragent_dowmloadmiddleware.py:5: ScrapyDeprecationWarning: Module `scrapy.contrib.downloadermiddleware.useragent` is deprecated, use `scrapy.downloadermiddlewares.useragent` instead
  from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware

2017-03-04 21:53:06 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'tc_zufang.rotate_useragent_dowmloadmiddleware.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-03-04 21:53:06 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-03-04 21:53:07 [py.warnings] WARNING: F:\githubcode\tc_zufang\tc_zufang\mongodb_pipeline.py:3: ScrapyDeprecationWarning: Module `scrapy.log` has been deprecated, Scrapy now relies on the builtin Python library for logging. Read the updated logging entry in the documentation to learn more.
  from scrapy import log

2017-03-04 21:53:07 [scrapy] INFO: Enabled item pipelines:
['tc_zufang.mongodb_pipeline.SingleMongodbPipeline']
2017-03-04 21:53:07 [scrapy] INFO: Spider opened
2017-03-04 21:53:07 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-03-04 21:53:07 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-03-04 21:53:07 [scrapy] DEBUG: Crawled (200) <GET http://dg.ganji.com/robots.txt> (referer: None)
2017-03-04 21:53:10 [scrapy] DEBUG: Crawled (200) <GET http://dg.ganji.com/fang1/> (referer: None)
2017-03-04 21:53:10 [scrapy] INFO: Closing spider (finished)
2017-03-04 21:53:10 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 436,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 40287,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 3, 4, 13, 53, 10, 985000),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'log_count/WARNING': 3,
 'response_received_count': 2,
 'scheduler/dequeued/redis': 1,
 'scheduler/enqueued/redis': 1,
 'start_time': datetime.datetime(2017, 3, 4, 13, 53, 7, 511000)}
2017-03-04 21:53:10 [scrapy] INFO: Spider closed (finished)
